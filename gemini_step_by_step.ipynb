{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Having Access to `MDLS_API_KEY` from `.env` File\n",
    "\n",
    "With this approach, you avoid writing an API key in the code. Instead, you put it in the `.env` file. The code below reads the variables in the `.env` file and sets them as environment variables.\n",
    "In this particular case, we only defined one variable: `MDLS_API_KEY`.\n",
    "\n",
    "**Important**: put the `.env` file in the same location as this file, or the file that will call the LLM. For example, if the LLM is going to be called from an AHK file, then place the `.env` file in the same location as the AHK file.\n",
    "\n",
    "The contents of the `.env` file should look like this:\n",
    "```plaintext\n",
    "Key_1=Value_1\n",
    "Key_2=Value_2\n",
    "...\n",
    "Key_n=Value_n\n",
    "```\n",
    "**To check use:** \n",
    "```plaintext\n",
    "    1. print(os.environ.get(\"MDLS_API_KEY\")), or \n",
    "    2. print(os.environ[\"MDLS_API_KEY\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read variables from .env file and sets them as environment variables.\n",
    "with open(\".env\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Get the environment var MDLS_API_KEY and use it in a print statement to check. \n",
    "# You can use it for anything you want, including to configure and use a LLM model :)\n",
    "# print(f'MDLS_API_KEY is: {os.environ[\"MDLS_API_KEY\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiates the model to generate Text from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai \n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"])\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Cuanto es la suma de 6 y 6?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiates the model to generate Text from Text & local Image   \n",
    "###### ***If using a single image, place the text prompt after the image.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai #\n",
    "import PIL.Image\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"]) #\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") #\n",
    "organ = PIL.Image.open(\"media/organ.jpg\")\n",
    "prompt = \"Tell me about this instrument\"\n",
    "response = model.generate_content([organ, prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instantiates the model to initiate a Chat using only Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai #\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"]) #\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") #\n",
    "chat = model.start_chat(\n",
    "    history=[\n",
    "        {\"role\": \"user\", \"parts\": \"Hello\"},\n",
    "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
    "    ]\n",
    ")\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Instantiates the model configuring **max_output_tokens** and **temperature** to generate content using only Text input   \n",
    "###### ***It's also possible to set config options independently in each generate_content request***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai #\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"]) #\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\", \n",
    "    generation_config=genai.types.GenerationConfig(max_output_tokens=10, temperature=0.0))\n",
    "\n",
    "response = model.generate_content(\"Tell me how many stars has the United States flag.\")\n",
    "print(response.text)\n",
    "\n",
    "response = model.generate_content(\"Tell me how many stars has the United States flag.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Instantiates the model configuring **temperature** and **system_instruction** to generate content using only Text input   \n",
    "###### ***It's also possible to set config options independently in each generate_content request***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai #\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"]) #\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\", \n",
    "    generation_config=genai.types.GenerationConfig(temperature=0.1), \n",
    "    system_instruction=\n",
    "    \"\"\"\n",
    "    You are an expert coder specialized in Python with more than 10 years of experience. You will help me with any python code that I need. Although you can read Spanish fluently, I may ask in spanish, but you will always answer in english.\n",
    "    Algunas veces querré que me des código para resolver una tarea puntual sin darte mayor detalle; en estos casos, usarás tu experiencia para interpretar mi requerimiento. Otras veces te daré la estructura de una función y su documentación, para que tu generes lo que falta para completar la función.\n",
    "     \n",
    "    Por ejemplo, si te paso:\n",
    "    def factorial(n:int) -> int:\n",
    "        '''\n",
    "        Calcula el factorial de un número entero no negativo usando recursividad.\n",
    "\n",
    "        Args:\n",
    "            n (int): Número entero no negativo para calcular el factorial.\n",
    "\n",
    "        Returns:\n",
    "            int: Factorial de n.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Si n es un número negativo.\n",
    "        '''\n",
    "\n",
    "    Tu respuesta será:\n",
    "        if n < 0:\n",
    "            raise ValueError(\"The Factorial is not defined for negative numbers.\")\n",
    "        elif n == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return n*factorial(n-1)\n",
    "\n",
    "    IMPORTANTE: Quiero el código sin comentarios.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    def fibonacci(n:int) -> int:\n",
    "        '''\n",
    "        Devuelve el n-ésimo número de la serie de fibonacci.\n",
    "\n",
    "        Args:\n",
    "            n (int): Número entero no negativo.\n",
    "\n",
    "        Returns:\n",
    "            int: el n-ésimo número de la serie de fibonacci.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Si n es un número negativo.\n",
    "        '''\n",
    "    \"\"\"\n",
    "    )\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Upload two PDF files to the cloud* so the LLM model can have access to them. The process finalize deleting the uploaded files   \n",
    "###### *The File API, where you have up to 20 GB space updated every 48 hours. Files can be up to 2 GB each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai #\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"]) #\n",
    "\n",
    "# Upload as many files as you need and be consistent below in response = model.generate_...\n",
    "myfile1 = genai.upload_file(\"media/PoderMAZG_Firmado.pdf\")\n",
    "myfile2 = genai.upload_file(\"media/PoderMAZG_Firmado2.pdf\")\n",
    "\n",
    "prompt = \"Summarize the differences between these documents.\"\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") #\n",
    "response = model.generate_content([prompt, myfile1, myfile2])\n",
    "print(response.text)\n",
    "\n",
    "# Delete the documents from the File API\n",
    "for f in genai.list_files():\n",
    "    f.delete()\n",
    "    print(f.name, \" - \", f.mime_type, \" deleted.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Instantiates the model, setting streaming response, for real-time content generation using only Text input\n",
    "\n",
    "This approach allows you to get responses as they are being generated, which is useful for creating more interactive applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"])\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "prompt = \"Write a story about a space adventure\"\n",
    "response = model.generate_content(prompt, stream=True)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Instantiates the model using safety settings to generate filtered content using only Text input\n",
    "\n",
    "This approach allows you to customize content filtering thresholds for generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"])\n",
    "\n",
    "safety_settings = {\n",
    "    \"harassment\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    \"hate_speech\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    \"sexually_explicit\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    \"dangerous\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    safety_settings=safety_settings\n",
    ")\n",
    "\n",
    "response = model.generate_content(\"Tell me the movies of Ilona Staller with a short description\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Instantiates the model to generate Text from Text & web Image   \n",
    "###### ***If using a single image, place the text prompt after the image.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a caption for the image:\n",
      "\n",
      "A panoramic view of London, England on an overcast day. The iconic landmarks of the London Eye, Big Ben, and the Shard are prominently featured, surrounded by a mix of historical and modern architecture. Lush green trees add a touch of nature to the urban landscape.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai #\n",
    "import httpx\n",
    "import os\n",
    "import base64\n",
    "\n",
    "genai.configure(api_key=os.environ[\"MDLS_API_KEY\"]) #\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") #\n",
    "\n",
    "image1_path = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall.jpg/2560px-Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall.jpg\"\n",
    "image1 = {'mime_type':'image/jpeg', 'data': base64.b64encode(httpx.get(image1_path).content).decode('utf-8')}\n",
    "\n",
    "prompt = \"Caption this image.\"\n",
    "\n",
    "response = model.generate_content([image1, prompt])\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
